{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9631465a",
   "metadata": {},
   "source": [
    "# 分类模型建模调参"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98738e5b",
   "metadata": {},
   "source": [
    "## 【任务背景】    \n",
    "   - **逻辑回归logistic regression：**                      \n",
    "   说到分类问题与回归问题的区别，在于回归问题与分类问题需要预测的因变量不一样。在回归问题中，因变量是连续性变量，我们需要预测$E(Y|X)$是一个连续的实数，但是在分类问题中，我们往往是通过已知X的信息预测Y的类别，往往是一个离散集合中的某个元素。如：是否患癌症，图片是猫还是狗等。一个很自然的想法是能否用线性回归去处理分类问题，答案是可以但不好！先来看看线性回归处理分类问题会出现什么弊端，我们仔细来看这个线性回归的例子，${default = \\beta_0 + \\beta_1 Balance + \\beta_2 Income}$，只要输入Balance 和 Income 以及default的数据就能用最小二乘法估计出${\\beta_0,\\beta_1}$,设定预测的default>0.5就是违约反之不违约，感觉很完美的样子，但事实真的是这样吗？假设我们需要用某个人的债务(Balance)和收入(Income)去预测是否会信用卡违约(default)：       \n",
    "      - 我们假设有一个穷人Lisa,他的Balance和Income都很小，那么有可能会导致default的值为负数，那么这个负数代表什么意义呢？显然是没有任何意义的。                \n",
    "      ![jupyter](./1.23.png)                            \n",
    "      - 当我们的分类变量是多类的时候，以0.5为界限划分分类就不可用了，那么我们应该怎么找到一个界限衡量多分类呢？                              \n",
    "   基于以上问题，现在大家是否还觉得线性回归模型作为一个分类模型是否足够优秀呢？其实，为了解决以上的问题（1）我们来想想能不能将线性回归的结果default转化为区间[0:1]上，让default转变成一个违约的概率呢？下面我们来解决这个问题吧。                              \n",
    "   在推导逻辑回归之前，我们先来认识下一组函数，这组函数具有神奇的作用，可以将是实数轴上的数转换为[0:1]区间上的概率。\n",
    "  首先，我们假设我们的线性回归模型为 ${Y=\\beta_0+\\beta_1 X}$，那么这个函数是如何将线性回归的结果转化为概率呢？这个函数就是logistic 函数，具体的形式为   ${p(X) = \\dfrac{e^{\\beta_0 + \\beta_1X}}{1+e^{\\beta_0 + \\beta_1X}}}$，他的函数图像如下图：（左边是线性回归，右边是逻辑函数）                             \n",
    "  ![jupyter](./1.24.png)                                   \n",
    "  因此，我们假设逻辑回归模型为：$p(y = 1|x) = \\frac{1}{1+e^{-w^Tx}}$ .                              \n",
    "  下面我们来具体推导下逻辑回归模型：                          \n",
    "  假设数据Data$\\{(x_i,y_i) \\},\\;\\;i = 1,2,...,N,\\;\\;x_i \\in R^p,y_i \\in \\{0,1 \\}$，设$p_1 = p(y=1|x) = \\sigma(w^T) = \\frac{1}{1+e^{-w^Tx}}$。因为y只可能取0或者1，因此假设数据服从0-1分布，也叫伯努力分布，即：当y=1时，$p(y|x)=p_1$，当y=0时，$p(y|x)=1-p_1$，可以写成$p(y|x) = p_1^y(1-p_1)^{1-y}$，可以带入y=0和y=1进去验证，结果和前面的结论一模一样。                    \n",
    "  我们使用极大似然估计MLE，即：                         \n",
    "  $$\n",
    "  \\hat{w} = argmax_w\\;\\;log\\;P(Y|X) = argmax_x\\;\\;log\\;\\prod_{i=1}^N P(y_i|x_i) = argmax_w \\sum\\limits_{i=1}^{N} log\\;P(y_i|x_i)\\\\\n",
    "  \\;\\;\\; = argmax_w \\sum\\limits_{i=1}^{N}(y_ilog\\;p_1 + (1-y_i)log(1-p_1)) \\\\ \n",
    "  记：L(w) = \\sum\\limits_{i=1}^{N}(y_ilog\\;p_1 + (1-y_i)log(1-p_1))\\\\\n",
    " \\;\\;\\; \\frac{\\partial L}{\\partial w_k} = \\sum\\limits_{i=1}^{N} y_i\\frac{1}{p_1}\\frac{\\partial p_1}{\\partial z}\\frac{\\partial z}{\\partial w_k} + (1-y_i)\\frac{1}{1-p_1}(-\\frac{\\partial p_1}{\\partial z}\\frac{\\partial z}{\\partial w_k})\\\\\n",
    "  \\;\\;\\;=\\sum\\limits_{i=1}^{N}y_i\\frac{1}{\\sigma(z)}(\\sigma(z_i)-\\sigma(z_i)^2)x_i + (1-y_i)\\frac{1}{1-\\sigma(z_i)}[-(\\sigma(z_i)-\\sigma(z_i)^2)x_i]\\\\\n",
    "  \\;\\;\\; =\\sum\\limits_{i=1}^{N}[(y_i-y_i\\sigma(z_i))x_i + (1-y_i)(-\\sigma(z_i))x_i]\\\\\n",
    "  \\;\\;\\; = \\sum\\limits_{i=1}^{N}y_ix_i-\\sigma(z_i)x_i = \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i\n",
    "  $$                 \n",
    "  因此，$\\frac{\\partial L}{\\partial w_k} = \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i$，由于这里涉及的函数不像线性回归一样能简单求出解析解，因此我们使用迭代的优化算法：梯度下降法，即：                       \n",
    "  $w_k^{(t+1)}\\leftarrow w_k^{(t)} - \\eta \\sum\\limits_{i=1}^{N}(y_i-\\sigma(z_i))x_i^{(k)},\\;\\;\\;其中，x_i^{(k)}为第i个样本第k个特征$                                 \n",
    " 值得注意的是，逻辑回归在实际中不太用于多分类问题，因为实际效果不是很好，所以我们可以借助其他模型来解决这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96caf397",
   "metadata": {},
   "source": [
    "   - 基于概率的分类模型：                               \n",
    "   (1) 线性判别分析：                                              \n",
    "   线性判别分析是一个比较久远的算法，我将会从两个方向去描述这个算法，因为我觉得每位读者都有自己喜欢的那个理解的方向，分别是基于贝叶斯公式和降维分类的思想。                        \n",
    "      - 基于贝叶斯公式对线性判别分析的理解：                       \n",
    "   在讨论如何解决多分类问题之前，我们先来说说贝叶斯的那些事吧。在概率统计的领域里有一条神奇的公式叫贝叶斯定理，具体的形式是：${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$ ，我们 先不要被公式的符号吓到，我们先来看看符号具体代表什么意思。我们假设观测有${K}$类，${\\pi_k}$为随机选择的观测来自第${k}$类的 __先验概率__，也就是样本里面第${k}$类的样本个数除以总样本的个数：${\\pi_k = \\dfrac{n_k}{n}}$。再来 ${f_k(x) =P(X=x|Y=k)}$，表示第${k}$类观测的X的密度函数，说的直白一点就是在${Y=k}$的样本里${X=x}$的样本个数，即${f_k(x) = P(X=x|Y=k) = \\dfrac{n_{(X=x,Y=k)}}{n_{(Y=k)}}}$，最后，${\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}=P(X=x)=\\dfrac{n_{(X=x)}}{n}$，也就是样本中${X=x}$的概率。\n",
    "      在讨论贝叶斯定理后，我们回到分类问题，这个定理跟我们的分类问题有什么关联呢？没错，这个公式${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$给出了给定样本条件下，${Y=k}$这个类别下的概率，这给分类问题提供了一条思路，那就是计算这个${P(Y=k|X=x)}$，而且我们的逻辑回归就是这么干的，但是在${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$这个公式中，分母${{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)} = P(X=x)}$当样本给定的时候是一个与分类${k}$无关的常数,所以我们的问题可以简化为只需要计算分子${{\\pi}_kf_k(x)}$,进而比较哪个类别的概率最大就知道属于哪个类别了，因此我们的分类思路就出来啦，这个思路不同于逻辑回归，逻辑回归需要计算具体的${P(Y=k|X=x)}$概率值，而我们现在的思路是通过贝叶斯定理计算贝叶斯定理的分子，比较分子最大的那个类别为最终类别。                 \n",
    "      在我们推导复杂算法之前，我们先推导下简单的当自变量个数只有一个的模型，即${p=1}$的简单模型。我们记${P(Y=k|X=x) = \\dfrac{{\\pi}_kf_k(x)}{\\sum\\limits_{l=1}^K{\\pi}_lf_l(x)}}$ 的分子为${g_k(x) = {\\pi}_kf_k(x)}$。在这里，我们做个模型假设：假设${f_k(x) }$服从正态分布，即${f_k(x) \\sim N(\\mu,\\sigma_k^2)}$，而且每个${\\sigma_k^2 = \\sigma^2}$，同方差假设。因此${f_k(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\dfrac{1}{2\\sigma^2}(x-\\mu_k)^2}}$，最终我们的${g_k(x) = \\pi_k\\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\dfrac{1}{2\\sigma^2}(x-\\mu_k)^2}}$,终于算出来啦。这个式子不是很好计算，我们对${g_k(x)}$取个对数，令${\\delta_k(x) = ln(g_k(x))=ln\\pi_k+\\dfrac{\\mu}{\\sigma^2}x-\\dfrac{\\mu^2}{2\\sigma^2}}$，到这里我们的模型建立模型，我们只需要把位置的${\\mu_k}$与${\\sigma^2}$估计出来就好了。${\\hat{\\mu}_k =\\dfrac{1}{n_k}\\sum\\limits_{i:y_i=k}x_i}$，也就是当${y=k}$这一类中${x}$的平均值；${\\hat{\\sigma}^2 =\\dfrac{1}{n-K}\\sum\\limits_{k=1}^K\\sum\\limits_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2 }$，说白了就是计算每一类的方差，再求平均值。总结下上面的公式就是：                                    \n",
    "${\\begin{cases}\\delta_k(x) = ln(g_k(x))=ln\\pi_k+\\dfrac{\\mu}{\\sigma^2}x-\\dfrac{\\mu^2}{2\\sigma^2}\\\\{\\hat{\\mu}_k =\\dfrac{1}{n_k}\\sum\\limits_{i:y_i=k}x_i}\\\\{\\hat{\\sigma}^2 =\\dfrac{1}{n-K}\\sum\\limits_{k=1}^K\\sum\\limits_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2}\\end{cases}}$                              \n",
    "      至此，我们的模型就建立完成了，我们只需要代入数据求出${\\delta_k(x)}$，哪个${k}$对应的${\\delta_k(x)}$大，就是哪一类。                                   \n",
    "   （下图虚线是线性判别分析的决策边界，正态曲线哪边高样本就是哪一类）                  \n",
    "      ![jupyter](./1.25.png)                            \n",
    "      我们推到出了一个自变量的简单模型，就要泛化为多个自变量的线性判别分析了，即${p>1}$。其实原理一样的，只是将一元正态分布扩展为多元正态分布：\n",
    "      ${f_k(x)=\\dfrac{1}{(2\\pi)^{\\tfrac{p}{2}}|\\Sigma|^\\tfrac{1}{2}}e^{[-\\tfrac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)]}}$                           \n",
    "      ${\\hat{\\mu_k}=(\\mu_{k1},\\mu_{k2},......,\\mu_{kp})   ,   \\hat{\\Sigma}=\\dfrac{1}{p-1}\\sum\\limits_{j=1}^p(x_j-\\overline{x})(x_j-\\overline{x})^T}$                               \n",
    "      ${\\delta_k(x) = ln(\\pi_kf_k(x))=ln(\\pi_k)-(\\dfrac{p}{2}ln(2\\pi)+\\dfrac{1}{2}ln(|\\Sigma|))-\\dfrac{1}{2}(x-\\mu_k)^T\\Sigma^-1(x-\\mu_k)=x^T\\hat{\\Sigma}\\hat{\\mu}_k-\\dfrac{1}                                                       {2}\\hat{\\mu}_k^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k+ln\\hat{\\pi}_k}$                            \n",
    "      - 降维分类的思想理解线性判别分析：                   \n",
    "      基于数据进行分类时，一个很自然的想法是：将高维的数据降维至一维，然后使用某个阈值将各个类别分开。下面用图的形式展示：                   \n",
    "      ![jupyter](./1.26.png)                        \n",
    "      图中，数据的维度是二维的，我们的想法是把数据降维至一维，然后用阈值就能分类。这个似乎是一个很好的想法，我们总是希望降维后的数据同一个类别自身内部方差小，不同类别之间的方差要尽可能大。这也是合理的，因为同一个类别的数据应该更加相似，因此方差小；不同类别的数据之间应该很不相似，这样才能更容易对数据进行分类，我们简称为：**类内方差小，类间方差大**，在计算机语言叫“松耦合，高内聚”。在做具体的推导之前，我们对数据的形式和一些基本统计量做一些描述：                            \n",
    "      特征$X = (x_1,x_2,...,x_N)^T$，因变量$Y = (y_1,y_2,...,y_N)^T,\\;\\;其中，y_i \\in \\{+1,-1 \\}$，类别c1的特征$X_{c_1} = \\{x_i|y_i=+1 \\}$，同理，类别c2的特征$X_{c_2} = \\{x_i|y_i=-1 \\}$，属于c1类别的数据个数为$N_1$，属于类别c2的数据个数为$N_2$，其中，$N_1+N_2 = N$。                         \n",
    "      特征X投影在w方向至一维：$z_i = w^Tx_i,\\;\\;||w|| = 1$                            \n",
    "      全样本投影的均值$\\bar{z} = \\frac{1}{N}\\sum\\limits_{i=1}^{N}z_i = \\frac{1}{N}\\sum\\limits_{i=1}^{N}w^Tx_i$                    \n",
    "      全样本投影的协方差$S_z = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(z_i-\\bar{z})(z_i-\\bar{z})^T = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(w^Tx_i-\\bar{z})(w^Tx_i-\\bar{z})^T$                   \n",
    "      c1样本投影的均值$\\bar{z_1} = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}z_i = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}w^Tx_i$                    \n",
    "      c1样本投影的协方差$S_{z_1} = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(z_i-\\bar{z_1})(z_i-\\bar{z_1})^T = \\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(w^Tx_i-\\bar{z_1})(w^Tx_i-\\bar{z_1})^T$                       \n",
    "      c2样本投影的均值 $\\bar{z_2} = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}z_i = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}w^Tx_i$                     \n",
    "      c2样本投影的协方差$S_{z_2} = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(z_i-\\bar{z_2})(z_i-\\bar{z_2})^T = \\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(w^Tx_i-\\bar{z_2})(w^Tx_i-\\bar{z_2})^T$                      \n",
    "      类间差距：$(\\bar{z}_1-\\bar{z}_2)^2$                      \n",
    "      类内方差：$S_1 + S_2$                          \n",
    "      由于线性判别分析的目标是同一类别内方差小，不同类别之间距离大，因此损失函数定义为：   \n",
    "                            \n",
    "   $$\n",
    "      J(w) = \\frac{(\\bar{z}_1-\\bar{z}_2)^2}{s_1+s_2} = \\frac{w^T(\\bar{x}_{c_1}-\\bar{x}_{c_2})(\\bar{x}_{c_1}-\\bar{x}_{c_2})^Tw}{w^T(s_{c_1}+s_{c_2})w}\\\\\n",
    "      \\;\\;\\; \\hat{w} = argmax_w\\;J(w)\n",
    "   $$                             \n",
    "   记：$S_b = (\\bar{x}_{c_1}-\\bar{x}_{c_2})(\\bar{x}_{c_1}-\\bar{x}_{c_2})^T,\\;S_w = (s_{c_1}+s_{c_2})$，因此$J(w) = \\frac{w^TS_bw}{w^TS_ww}$                   \n",
    "   让J(w)对w求导等于0，求出：$w = S_w^{-1}(\\bar{x}_{c_1}-\\bar{x}_{c_2})$                       \n",
    "   (2) 朴素贝叶斯：                                        \n",
    "   在线性判别分析中，我们假设每种分类类别下的特征遵循同一个协方差矩阵，每两个特征之间是存在协方差的，因此在线性判别分析中各种特征是不是独立的。但是，朴素贝叶斯算法对线性判别分析作进一步的模型简化，它将线性判别分析中的协方差矩阵中的协方差全部变成0，只保留各自特征的方差，也就是朴素贝叶斯假设各个特征之间是不相关的。在之前所看到的偏差-方差理论中，我们知道模型的简化可以带来方差的减少但是增加偏差，因此朴素贝叶斯也不例外，它比线性判别分析模型的方差小，偏差大。虽然简化了模型，实际中使用朴素贝叶斯的案例非常多，甚至多于线性判别分析，例如鼎鼎大名的新闻分类，垃圾邮件分类等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a76f0",
   "metadata": {},
   "source": [
    "   - 决策树 ：                     \n",
    "   与前面内容所讲的决策树回归大致是一样的，只是在回归问题中，选择分割点的标准是均方误差，但是在分类问题中，由于因变量是类别变量而不是连续变量，因此用均方误差显然不合适。那问题是用什么作为选择分割点的标准呢？我们先来分析具体的问题：                         \n",
    "   在回归树中，对一个给定的观测值，因变量的预测值取它所属的终端结点内训练集的平均因变量。与之相对应，对于分类树来说，给定一个观测值，因变量的预测值为它所属的终端结点内训练集的**最常出现的类**。分类树的构造过程与回归树也很类似，与回归树一样，分类树也是采用递归二叉分裂。但是在分类树中，均方误差无法作为确定分裂节点的准则，一个很自然的替代指标是分类错误率。分类错误率就是：此区域内的训练集中非常见类所占的类别，即：                                   \n",
    "   $$\n",
    "   E = 1-max_k(\\hat{p}_{mk})\n",
    "   $$                       \n",
    "   上式中的$\\hat{p}_{mk}$代表第m个区域的训练集中第k类所占的比例。但是在大量的事实证明：分类错误率在构建决策树时不够敏感，一般在实际中用如下两个指标代替：             \n",
    "   (1) 基尼系数：                   \n",
    "   $$\n",
    "   G = \\sum\\limits_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})\n",
    "   $$             \n",
    "   在基尼系数的定义中，我们发现这个指标衡量的是K个类别的总方差。不难发现，如果所有的$\\hat{p}_{mk}$的取值都接近0或者1，基尼系数会很小。因此基尼系数被视为衡量结点纯度的指标----如果他的取值小，那就意味着某个节点包含的观测值几乎来自同一个类别。                         \n",
    "   由基尼系数作为指标得到的分类树叫做：CART。                        \n",
    "   (2) 交叉熵：                       \n",
    "   可以替代基尼系数的指标是交叉熵，定义如下：                           \n",
    "   $$\n",
    "   D = -\\sum\\limits_{k=1}^{K} \\hat{p}_{mk}log\\;\\hat{p}_{mk}\n",
    "   $$                     \n",
    "   显然，如果所有的$\\hat{p}_{mk}$都接近于0或者1，那么交叉熵就会接近0。因此，和基尼系数一样，如果第m个结点的纯度越高，则交叉熵越小。事实证明，基尼系数和交叉熵在数值上时很接近的。                   \n",
    "   \n",
    "   ![jupyter](./1.27.png)                                            \n",
    "   决策树分类算法的完整步骤：                          \n",
    "      a.  选择最优切分特征j以及该特征上的最优点s：                \n",
    "      遍历特征j以及固定j后遍历切分点s，选择使得基尼系数或者交叉熵最小的(j,s)                                                   \n",
    "       b. 按照(j,s)分裂特征空间，每个区域内的类别为该区域内样本比例最多的类别。                           \n",
    "       c. 继续调用步骤1，2直到满足停止条件，就是每个区域的样本数小于等于5。        \n",
    "       d. 将特征空间划分为J个不同的区域，生成分类树。   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360404d",
   "metadata": {},
   "source": [
    "选择度量模型性能的指标：                                    \n",
    "度量分类模型的指标和回归的指标有很大的差异，首先是因为分类问题本身的因变量是离散变量，因此像定义回归的指标那样，单单衡量预测值和因变量的相似度可能行不通。其次，在分类任务中，我们对于每个类别犯错的代价不尽相同，例如：我们将癌症患者错误预测为无癌症和无癌症患者错误预测为癌症患者，在医院和个人的代价都是不同的，前者会使得患者无法得到及时的救治而耽搁了最佳治疗时间甚至付出生命的代价，而后者只需要在后续的治疗过程中继续取证就好了，因此我们很不希望出现前者，当我们发生了前者这样的错误的时候会认为建立的模型是很差的。为了解决这些问题，我们必须将各种情况分开讨论，然后给出评价指标。             \n",
    "   - 真阳性TP：预测值和真实值都为正例；                        \n",
    "   - 真阴性TN：预测值与真实值都为正例；                     \n",
    "   - 假阳性FP：预测值为正，实际值为负；\n",
    "   - 假阴性FN：预测值为负，实际值为正；                      \n",
    "   ![jupyter](./1.22.png)                                       \n",
    "分类模型的指标：                    \n",
    "   - 准确率：分类正确的样本数占总样本的比例，即：$ACC = \\frac{TP+TN}{FP+FN+TP+TN}$.                                \n",
    "   - 精度：预测为正且分类正确的样本占预测值为正的比例，即：$PRE = \\frac{TP}{TP+FP}$.                     \n",
    "   - 召回率：预测为正且分类正确的样本占类别为正的比例，即：$REC =  \\frac{TP}{TP+FN}$.                     \n",
    "   - F1值：综合衡量精度和召回率，即：$F1 = 2\\frac{PRE\\times REC}{PRE + REC}$.                                     \n",
    "   - ROC曲线：以假阳率为横轴，真阳率为纵轴画出来的曲线，曲线下方面积越大越好。                                                          \n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics                           \n",
    "![jupyter](./1.21.png)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18439093",
   "metadata": {},
   "source": [
    "## 【任务描述】   \n",
    "使用sklearn进行分类模型的建模和调参：  \n",
    "1. 掌握调用sklearn库中的预置模型建模的方法；\n",
    "2. 掌握常用的分类模型调参技巧；\n",
    "3. 能够对分类模型进行评估和优化。\n",
    "    \n",
    "## 【任务准备】    \n",
    "学员已完成分类数据特征工程实践课程，已保存完成特征工程的数据。        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d0806",
   "metadata": {},
   "source": [
    "## 【任务实施】    \n",
    "### 步骤 1: 启动Jupyter Notebook    \n",
    "+ 在搜索栏输入\"cmd\"命令，启动命令提示符窗口。    \n",
    "+ 输入\"jupyter notebook\"命令，并按回车键启动Jupyter Notebook。   \n",
    "    \n",
    "### 步骤 2: 创建新的Notebook    \n",
    "+ 在Jupyter的Web界面中，点击右上角的 \"New\" 按钮。    \n",
    "+ 选择 \"Python 3\"内核来创建一个新的Python 3 Notebook。    \n",
    "    \n",
    "### 步骤 3: 导入必要的库\n",
    "+ 使用numpy和pandas库来进行数据处理，使用sklearn、lightgbm和xgboost库来进行数据的建模预测和评估，使用matplotlib和seaborn来进行数据的可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0770b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "\n",
    "# 模型预测库\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 参数搜索和评价库\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "# 定义字体，在图表中正常显示汉字\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# 在图表中正常显示负号\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "#导入warnings包，利用过滤器来实现忽略警告语句。\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca7221",
   "metadata": {},
   "source": [
    "### 步骤 4: 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676df48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>是否幸存</th>\n",
       "      <th>仓位等级</th>\n",
       "      <th>兄弟姐妹个数</th>\n",
       "      <th>父母子女个数</th>\n",
       "      <th>票价</th>\n",
       "      <th>登船港口</th>\n",
       "      <th>年龄段</th>\n",
       "      <th>性别分类_female</th>\n",
       "      <th>性别分类_male</th>\n",
       "      <th>性别分类_unknown</th>\n",
       "      <th>客舱_labelEncode</th>\n",
       "      <th>船票信息_labelEncode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>715 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     是否幸存  仓位等级  兄弟姐妹个数  父母子女个数       票价  登船港口  年龄段  性别分类_female  性别分类_male  \\\n",
       "0       0     3       1       0   7.2500     1    2            0          1   \n",
       "1       1     1       1       0  71.2833     2    3            1          0   \n",
       "2       1     3       0       0   7.9250     1    2            1          0   \n",
       "3       1     1       1       0  53.1000     1    3            1          0   \n",
       "4       0     3       0       0   8.0500     1    3            0          1   \n",
       "..    ...   ...     ...     ...      ...   ...  ...          ...        ...   \n",
       "710     0     3       0       5  29.1250     3    3            1          0   \n",
       "711     0     2       0       0  13.0000     1    2            0          1   \n",
       "712     1     1       0       0  30.0000     1    2            1          0   \n",
       "713     1     1       0       0  30.0000     2    2            0          1   \n",
       "714     0     3       0       0   7.7500     3    3            0          1   \n",
       "\n",
       "     性别分类_unknown  客舱_labelEncode  船票信息_labelEncode  \n",
       "0               0             135               409  \n",
       "1               0              74               472  \n",
       "2               0             135               533  \n",
       "3               0              50                41  \n",
       "4               0             135               374  \n",
       "..            ...             ...               ...  \n",
       "710             0             135               378  \n",
       "711             0             135                84  \n",
       "712             0              27                13  \n",
       "713             0              54                 9  \n",
       "714             0             135               372  \n",
       "\n",
       "[715 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_FE = pd.read_csv('data_FE.csv')\n",
    "data_FE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7bcf2d",
   "metadata": {},
   "source": [
    "### 步骤 5: 切割训练集和测试集    \n",
    "* 按比例切割训练集和测试集(一般测试集的比例有30%、25%、20%、15%和10%)\n",
    "* 按目标变量分层进行等比切割\n",
    "* 设置随机种子以便结果能复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeda386f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((536, 11), (179, 11))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_FE.iloc[:,1:]\n",
    "y = data_FE.iloc[:,:1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "# 查看数据形状\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58c4a2",
   "metadata": {},
   "source": [
    "### 步骤 6: 模型创建     \n",
    "6.1 创建基于线性模型的分类模型（逻辑回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11dc33c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.80\n",
      "Testing set score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# 默认参数逻辑回归模型\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "# 查看训练集和测试集score值\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Testing set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41bcd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.81\n",
      "Testing set score: 0.77\n"
     ]
    }
   ],
   "source": [
    "# 调整参数后的逻辑回归模型\n",
    "lr2 = LogisticRegression(C=100)\n",
    "lr2.fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lr2.score(X_train, y_train)))\n",
    "print(\"Testing set score: {:.2f}\".format(lr2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54daaf",
   "metadata": {},
   "source": [
    "6.2 创建基于树的分类模型（随机森林）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674c5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.00\n",
      "Testing set score: 0.77\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(rfc.score(X_train, y_train)))\n",
    "print(\"Testing set score: {:.2f}\".format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a2e752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.87\n",
      "Testing set score: 0.80\n"
     ]
    }
   ],
   "source": [
    "# 调整参数后的随机森林分类模型\n",
    "rfc2 = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "rfc2.fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(rfc2.score(X_train, y_train)))\n",
    "print(\"Testing set score: {:.2f}\".format(rfc2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaaf029",
   "metadata": {},
   "source": [
    "### 步骤 7: 输出模型预测结果    \n",
    "* 输出模型预测分类标签\n",
    "* 输出不通分类标签的预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8403ff29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测标签\n",
    "pred = lr.predict(X_train)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2108b1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78673133, 0.21326867],\n",
       "       [0.89755559, 0.10244441],\n",
       "       [0.37928636, 0.62071364],\n",
       "       ...,\n",
       "       [0.6032195 , 0.3967805 ],\n",
       "       [0.43883354, 0.56116646],\n",
       "       [0.07981926, 0.92018074]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测标签概率\n",
    "pred_proba = lr.predict_proba(X_train)\n",
    "pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ab837",
   "metadata": {},
   "source": [
    "### 步骤 8: 模型评估    \n",
    "* 模型评估是为了知道模型的泛化能力。\n",
    "* 交叉验证（cross-validation）是一种评估泛化性能的统计学方法，它比单次划分训练集和测试集的方法更加稳定、全面。\n",
    "* 在交叉验证中，数据被多次划分，并且需要训练多个模型。\n",
    "* 最常用的交叉验证是 k 折交叉验证（k-fold cross-validation），其中 k 是由用户指定的数字，通常取 5 或 10。\n",
    "* 准确率（precision）度量的是被预测为正例的样本中有多少是真正的正例\n",
    "* 召回率（recall）度量的是正类样本中有多少被预测为正类\n",
    "* f-分数是准确率与召回率的调和平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9305c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72222222, 0.77777778, 0.75925926, 0.81481481, 0.68518519,\n",
       "       0.81481481, 0.86792453, 0.83018868, 0.88679245, 0.83018868])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=100)\n",
    "scores = cross_val_score(lr, X_train, y_train, cv=10)\n",
    "# k折交叉验证分数\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ca3ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score: 0.80\n"
     ]
    }
   ],
   "source": [
    "# 平均交叉验证分数\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54bb75",
   "metadata": {},
   "source": [
    "### 步骤 9: 混淆矩阵      \n",
    "* 计算二分类问题的混淆矩阵\n",
    "* 计算精确率、召回率以及f-分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2945d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[278,  41],\n",
       "       [ 62, 155]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "lr = LogisticRegression(C=100)\n",
    "lr.fit(X_train, y_train)\n",
    "# 模型预测结果\n",
    "pred = lr.predict(X_train)\n",
    "# 混淆矩阵\n",
    "confusion_matrix(y_train, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c0ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84       319\n",
      "           1       0.79      0.71      0.75       217\n",
      "\n",
      "    accuracy                           0.81       536\n",
      "   macro avg       0.80      0.79      0.80       536\n",
      "weighted avg       0.81      0.81      0.81       536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 精确率、召回率以及f1-score\n",
    "print(classification_report(y_train, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3c5c",
   "metadata": {},
   "source": [
    "### 步骤 10: 绘制ROC曲线    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114a65b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20bbcd397c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, lr.decision_function(X_test))\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "# 找到最接近于0的阈值\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10, label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e464caf",
   "metadata": {},
   "source": [
    "## 【任务总结】    \n",
    "做数据分析的目的，就是运用数据、结合业务来得到某些我们想要知道的结果。    \n",
    "分析的第一步就是建模，搭建一个预测模型或者其他模型。    \n",
    "从模型得到预测结果之后，还要要分析这个模型是不是足够可靠，这样就需要评估这个模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496beb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
